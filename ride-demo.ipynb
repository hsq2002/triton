{"cells":[{"cell_type":"code","execution_count":1,"id":"625d432b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"625d432b","executionInfo":{"status":"ok","timestamp":1758746593959,"user_tz":300,"elapsed":6516,"user":{"displayName":"tingxi li","userId":"07148518068737199447"}},"outputId":"81b87c0e-82f2-4a64-b11e-cf29d33cd030"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"]}],"source":["!pip install torch triton"]},{"cell_type":"code","execution_count":2,"id":"4662b919","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4662b919","executionInfo":{"status":"ok","timestamp":1758746598564,"user_tz":300,"elapsed":4602,"user":{"displayName":"tingxi li","userId":"07148518068737199447"}},"outputId":"d1d90263-dd40-4b5e-b111-6294e7d3e049"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}],"source":["import torch, triton\n","print(torch.cuda.is_available())\n","# it has to be True, triton does not support CPU!"]},{"cell_type":"code","execution_count":null,"id":"19cd6ec7","metadata":{"id":"19cd6ec7","outputId":"926bedba-0bf0-47ef-f5e2-c80f32761bf7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Correctness] Triton result matches PyTorch: True\n","[Perf] Triton: 0.660 ms  (~304.9 GB/s)\n","[Perf] PyTorch: 1.087 ms  (~185.1 GB/s)\n"]}],"source":["import torch\n","import triton\n","import triton.language as tl\n","\n","\n","# ---------------------------\n","# 1) The Triton kernel itself\n","# ---------------------------\n","\n","@triton.jit\n","def vector_add_kernel(\n","    A,  # *device* pointer to input vector A (float32 elements)\n","    B,  # *device* pointer to input vector B (float32 elements)\n","    C,  # *device* pointer to output vector C (float32 elements)\n","    N,  # total number of elements (int32 or int64 scalar)\n","    BLOCK_SIZE: tl.constexpr,  # compile-time constant: how many elements a single program handles\n","):\n","    \"\"\"\n","    A Triton kernel is a function annotated with @triton.jit.\n","    It will be JIT-compiled to GPU code (PTX) the first time you launch it.\n","\n","    Conceptual model:\n","      - Triton launches many \"programs\" in parallel on the GPU.\n","      - Each program is responsible for a *tile* (a small chunk) of the problem.\n","      - `tl.program_id(axis=0)` returns the id of the current program along the 1D grid.\n","    \"\"\"\n","\n","    # Identify which \"program\" (i.e., CUDA-like block) we are in along axis 0.\n","    pid = tl.program_id(axis=0)\n","\n","    # Within this program, we process a contiguous block of indices of length BLOCK_SIZE:\n","    # global indices = pid * BLOCK_SIZE + [0, 1, 2, ..., BLOCK_SIZE-1]\n","    # tl.arange(0, BLOCK_SIZE) creates a vector [0..BLOCK_SIZE-1] at *compile-time* (constexpr).\n","    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n","\n","    # Compute a boolean mask to guard against out-of-bounds when N is not a multiple of BLOCK_SIZE.\n","    # Any lane whose index >= N will be masked off (i.e., deactivated for loads/stores).\n","    mask = offsets < N\n","\n","    # Load A[offsets] and B[offsets] from global memory.\n","    # tl.load supports a `mask=` argument so masked-out lanes won't cause invalid memory access.\n","    # For masked lanes, tl.load returns 0 by default (can be overridden via `other=`).\n","    a = tl.load(A + offsets, mask=mask)\n","\n","    # Same for B.\n","    b = tl.load(B + offsets, mask=mask)\n","\n","    # Elementwise addition happens on registers; write results back to global memory.\n","    # Masked lanes won't store anything.\n","    tl.store(C + offsets, a + b, mask=mask)\n","\n","\n","# ---------------------------------\n","# 2) A small helper for benchmarking\n","# ---------------------------------\n","def time_op_gpu(fn, sync=True, warmup=5, iters=20):\n","    \"\"\"\n","    Time a GPU operation using CUDA events for better accuracy (no CPU scheduling noise).\n","    - fn: a callable that launches GPU work\n","    - sync: whether to synchronize after each iteration (True recommended)\n","    - warmup: warm-up iterations to let JIT/caches settle\n","    - iters: timed iterations\n","\n","    Returns: average time in milliseconds over 'iters' runs.\n","    \"\"\"\n","    # warm-up does JIT and warms caches\n","    for _ in range(warmup):\n","        fn()\n","    if sync:\n","        torch.cuda.synchronize()\n","\n","    start = torch.cuda.Event(enable_timing=True)\n","    end = torch.cuda.Event(enable_timing=True)\n","    elapsed_ms = 0.0\n","    for _ in range(iters):\n","        start.record()\n","        fn()\n","        end.record()\n","        # Wait for the events to be recorded & measure GPU time\n","        torch.cuda.synchronize()\n","        elapsed_ms += start.elapsed_time(end)\n","    return elapsed_ms / iters\n","\n","\n","# ---------------\n","# 3) Driver code\n","# ---------------\n","def main():\n","    assert torch.cuda.is_available(), \"CUDA device not found. Please run on a machine with an NVIDIA GPU.\"\n","\n","    # Problem size: try something reasonably large to see speed differences.\n","    N = 1 << 24  # ~16 million elements\n","\n","    # Allocate inputs/outputs directly on the GPU.\n","    # Triton interoperates smoothly with PyTorch tensors as raw pointers are passed under the hood.\n","    a = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n","    b = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n","    c = torch.empty_like(a)\n","\n","    # Choose how much work each program handles. Typical sizes: 128/256/512/1024.\n","    # Larger BLOCK_SIZE improves memory coalescing and reduces launch overhead but may reduce occupancy\n","    # if register/shared-mem pressure is high (not a big concern for this simple kernel).\n","    BLOCK_SIZE = 1024\n","\n","    # Grid definition: how many programs to launch? We need one program per \"tile\" of size BLOCK_SIZE.\n","    # triton.cdiv(x, y) = ceil_div(x, y).\n","    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE\"]),)\n","\n","    # 3.1) Correctness check ---------------------------------------------------\n","    vector_add_kernel[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n","    # Synchronize to ensure kernel has finished before checking results.\n","    torch.cuda.synchronize()\n","    ok = torch.allclose(c, a + b, rtol=1e-5, atol=1e-6)\n","    print(f\"[Correctness] Triton result matches PyTorch: {ok}\")\n","\n","    # 3.2) Performance: Triton vs PyTorch -------------------------------------\n","    # Define a launcher for Triton (so our timing helper can call it repeatedly).\n","    def launch_triton():\n","        vector_add_kernel[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n","\n","    # PyTorch baseline (also launches a highly optimized GPU kernel internally).\n","    def launch_torch():\n","        # We write to c to emulate the same output pattern as Triton; in practice you might reuse ref = a + b.\n","        c.copy_(a + b)\n","\n","    triton_ms = time_op_gpu(launch_triton)\n","    torch_ms = time_op_gpu(launch_torch)\n","\n","    # Throughput: number of bytes moved per second.\n","    # Each element reads A[i] and B[i] (2 * 4 bytes) and writes C[i] (4 bytes) => 12 bytes/element.\n","    bytes_moved = N * 12\n","    triton_bw = bytes_moved / (triton_ms / 1e3) / 1e9  # GB/s\n","    torch_bw = bytes_moved / (torch_ms / 1e3) / 1e9    # GB/s\n","\n","    print(f\"[Perf] Triton: {triton_ms:.3f} ms  (~{triton_bw:.1f} GB/s)\")\n","    print(f\"[Perf] PyTorch: {torch_ms:.3f} ms  (~{torch_bw:.1f} GB/s)\")\n","\n","    # 3.3) (Optional) Try different BLOCK_SIZE to see the effect on performance\n","    # for bs in [128, 256, 512, 1024, 2048]:\n","    #     grid2 = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE\"]),)\n","    #     def launch():\n","    #         vector_add_kernel[grid2](a, b, c, N, BLOCK_SIZE=bs)\n","    #     t_ms = time_op_gpu(launch)\n","    #     bw = bytes_moved / (t_ms / 1e3) / 1e9\n","    #     print(f\"BLOCK_SIZE={bs:4d}  ->  {t_ms:.3f} ms  (~{bw:.1f} GB/s)\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"69fa8d6d","metadata":{"id":"69fa8d6d"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}