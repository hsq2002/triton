{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPToEIv08UJV+c9uq0ASade",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsq2002/triton/blob/main/triton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-eXZpMnzWAm",
        "outputId": "5b4e4bc2-db59-4a39-cb37-d4243e1dab43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, triton\n",
        "#checks availability of a GPU\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "else: \"None\"\n",
        "# it has to be True, triton does not support CPU!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe5e7KGeza09",
        "outputId": "95fdf78c-4021-47de-c35e-177b9deeb56c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 1) The Triton kernel itself (A + B = C)\n",
        "# ---------------------------\n",
        "\n",
        "@triton.jit\n",
        "def vector_add_kernel(\n",
        "    A,  # *device* pointer to input vector A (float32 elements)\n",
        "    B,  # *device* pointer to input vector B (float32 elements)\n",
        "    C,  # *device* pointer to output vector C (float32 elements)\n",
        "    N,  # total number of elements (int32 or int64 scalar)\n",
        "    BLOCK_SIZE: tl.constexpr,  # compile-time constant: how many elements a single program handles\n",
        "):\n",
        "    \"\"\"\n",
        "    A Triton kernel is a function annotated with @triton.jit.\n",
        "    It will be JIT-compiled to GPU code (PTX) the first time you launch it.\n",
        "\n",
        "    Conceptual model:\n",
        "      - Triton launches many \"programs\" in parallel on the GPU.\n",
        "      - Each program is responsible for a *tile* (a small chunk) of the problem.\n",
        "      - `tl.program_id(axis=0)` returns the id of the current program along the 1D grid.\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify which \"program\" (i.e., CUDA-like block) we are in along axis 0.\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # Within this program, we process a contiguous block of indices of length BLOCK_SIZE:\n",
        "    # global indices = pid * BLOCK_SIZE + [0, 1, 2, ..., BLOCK_SIZE-1]\n",
        "    # tl.arange(0, BLOCK_SIZE) creates a vector [0..BLOCK_SIZE-1] at *compile-time* (constexpr).\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "    # Compute a boolean mask to guard against out-of-bounds when N is not a multiple of BLOCK_SIZE.\n",
        "    # Any lane whose index >= N will be masked off (i.e., deactivated for loads/stores).\n",
        "    mask = offsets < N\n",
        "\n",
        "    # Load A[offsets] and B[offsets] from global memory.\n",
        "    # tl.load supports a `mask=` argument so masked-out lanes won't cause invalid memory access.\n",
        "    # For masked lanes, tl.load returns 0 by default (can be overridden via `other=`).\n",
        "    a = tl.load(A + offsets, mask=mask)\n",
        "\n",
        "    # Same for B.\n",
        "    b = tl.load(B + offsets, mask=mask)\n",
        "\n",
        "    # Elementwise addition happens on registers; write results back to global memory.\n",
        "    # Masked lanes won't store anything.\n",
        "    tl.store(C + offsets, a + b, mask=mask)\n",
        "# ---------------------------------\n",
        "# 2) Added Kernel #1: C = alpha * A + B\n",
        "# ---------------------------------\n",
        "@triton.jit\n",
        "def alpha_add_kernel(A, B, C, alpha, N, BLOCK_SIZE: tl.constexpr):\n",
        "  pid = tl.program_id(axis=0)\n",
        "  offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "  mask = offsets < N\n",
        "  a = tl.load(A + offsets, mask=mask)\n",
        "  b = tl.load(B + offsets, mask=mask)\n",
        "  tl.store(C + offsets, alpha * a + b, mask=mask)\n",
        "\n",
        "# ---------------------------------\n",
        "# 2) Added Kernel #2: C = A * B\n",
        "# ---------------------------------\n",
        "@triton.jit\n",
        "def element_wise_matrix_multiplication(A, B, C, N, BLOCK_SIZE: tl.constexpr):\n",
        "  pid = tl.program_id(axis=0)\n",
        "  offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "  mask = offsets < N\n",
        "  a = tl.load(A + offsets, mask=mask)\n",
        "  b = tl.load(B + offsets, mask=mask)\n",
        "  tl.store(C + offsets, a * b, mask=mask)\n",
        "\n",
        "# ---------------------------------\n",
        "# 4) A small helper for benchmarking\n",
        "# ---------------------------------\n",
        "def time_op_gpu(fn, sync=True, warmup=5, iters=20):\n",
        "    \"\"\"\n",
        "    Time a GPU operation using CUDA events for better accuracy (no CPU scheduling noise).\n",
        "    - fn: a callable that launches GPU work\n",
        "    - sync: whether to synchronize after each iteration (True recommended)\n",
        "    - warmup: warm-up iterations to let JIT/caches settle\n",
        "    - iters: timed iterations\n",
        "\n",
        "    Returns: average time in milliseconds over 'iters' runs.\n",
        "    \"\"\"\n",
        "    # warm-up does JIT and warms caches\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    if sync:\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    elapsed_ms = 0.0\n",
        "    for _ in range(iters):\n",
        "        start.record()\n",
        "        fn()\n",
        "        end.record()\n",
        "        # Wait for the events to be recorded & measure GPU time\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed_ms += start.elapsed_time(end)\n",
        "    return elapsed_ms / iters\n",
        "\n",
        "\n",
        "# ---------------\n",
        "# 5) Driver code\n",
        "# ---------------\n",
        "def main():\n",
        "    assert torch.cuda.is_available(), \"CUDA device not found. Please run on a machine with an NVIDIA GPU.\"\n",
        "\n",
        "    # Problem size: try something reasonably large to see speed differences.\n",
        "    N = 1 << 24  # ~16 million elements\n",
        "\n",
        "    # Allocate inputs/outputs directly on the GPU.\n",
        "    # Triton interoperates smoothly with PyTorch tensors as raw pointers are passed under the hood.\n",
        "    alpha = 3.0\n",
        "    a = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    b = torch.rand(N, device=\"cuda\", dtype=torch.float32)\n",
        "    c = torch.empty_like(a)\n",
        "    #changing the device type out of cuda to CPU breaks the program\n",
        "\n",
        "    # Choose how much work each program handles. Typical sizes: 128/256/512/1024.\n",
        "    # Larger BLOCK_SIZE improves memory coalescing and reduces launch overhead but may reduce occupancy\n",
        "    # if register/shared-mem pressure is high (not a big concern for this simple kernel).\n",
        "    BLOCK_SIZE = 1024\n",
        "\n",
        "    # Grid definition: how many programs to launch? We need one program per \"tile\" of size BLOCK_SIZE.\n",
        "    # triton.cdiv(x, y) = ceil_div(x, y).\n",
        "    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE\"]),)\n",
        "\n",
        "    # --- Run kernels ----\n",
        "    print(\"Running kernels...\\n\")\n",
        "    print(\"Add Kernel:\")\n",
        "    vector_add_kernel[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "    print(\"Alpha Kernel (alpha*A + B):\")\n",
        "    alpha_add_kernel[grid](a, b, c, alpha, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "    print(\"Multiplication Kernel (A * B):\")\n",
        "    element_wise_matrix_multiplication[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # 3.1) Correctness check ---------------------------------------------------\n",
        "    print(\"\\n[Correctness Checks]\")\n",
        "    vector_add_kernel[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "    print(\"A + B:\", torch.allclose(c, a + b))\n",
        "\n",
        "    alpha_add_kernel[grid](a, b, c, alpha, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "    print(\"alpha*A + B:\", torch.allclose(c, alpha * a + b))\n",
        "\n",
        "    element_wise_matrix_multiplication[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "    print(\"A * B:\", torch.allclose(c, a * b))\n",
        "    # Synchronize to ensure kernel has finished before checking results.\n",
        "    torch.cuda.synchronize()\n",
        "    ok = torch.allclose(c, a + b, rtol=1e-5, atol=1e-6)\n",
        "    print(f\"[Correctness] Triton result matches PyTorch: {ok}\")\n",
        "\n",
        "    #print result\n",
        "    print(\"\\nFirst 5 values after multiplication:\")\n",
        "    print(\"alpha      =\", alpha)\n",
        "    print(\"a[:5]      =\", a[:5].cpu())\n",
        "    print(\"b[:5]      =\", b[:5].cpu())\n",
        "    print(\"c[:5]      =\", c[:5].cpu())\n",
        "    print(\"(a*b)[:5]  =\", (a*b)[:5].cpu())\n",
        "\n",
        "    # 3.2) Performance: Triton vs PyTorch -------------------------------------\n",
        "    # Define a launcher for Triton (so our timing helper can call it repeatedly).\n",
        "    def launch_triton():\n",
        "        vector_add_kernel[grid](a, b, c, N, BLOCK_SIZE=BLOCK_SIZE)\n",
        "\n",
        "\n",
        "    # PyTorch baseline (also launches a highly optimized GPU kernel internally).\n",
        "    def launch_torch():\n",
        "        # We write to c to emulate the same output pattern as Triton; in practice you might reuse ref = a + b.\n",
        "        c.copy_(a + b)\n",
        "\n",
        "    triton_ms = time_op_gpu(launch_triton)\n",
        "    torch_ms = time_op_gpu(launch_torch)\n",
        "\n",
        "    # Throughput: number of bytes moved per second.\n",
        "    # Each element reads A[i] and B[i] (2 * 4 bytes) and writes C[i] (4 bytes) => 12 bytes/element.\n",
        "    bytes_moved = N * 12\n",
        "    triton_bw = bytes_moved / (triton_ms / 1e3) / 1e9  # GB/s\n",
        "    torch_bw = bytes_moved / (torch_ms / 1e3) / 1e9    # GB/s\n",
        "\n",
        "    print(f\"\\n[Performance]\")\n",
        "    print(f\"[Perf] Triton: {triton_ms:.3f} ms  (~{triton_bw:.1f} GB/s)\")\n",
        "    print(f\"[Perf] PyTorch: {torch_ms:.3f} ms  (~{torch_bw:.1f} GB/s)\")\n",
        "\n",
        "    # 3.3) (Optional) Try different BLOCK_SIZE to see the effect on performance\n",
        "    # for bs in [128, 256, 512, 1024, 2048]:\n",
        "    #     grid2 = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE\"]),)\n",
        "    #     def launch():\n",
        "    #         vector_add_kernel[grid2](a, b, c, N, BLOCK_SIZE=bs)\n",
        "    #     t_ms = time_op_gpu(launch)\n",
        "    #     bw = bytes_moved / (t_ms / 1e3) / 1e9\n",
        "    #     print(f\"BLOCK_SIZE={bs:4d}  ->  {t_ms:.3f} ms  (~{bw:.1f} GB/s)\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1VLQqz_0o_j",
        "outputId": "aee6895e-e58a-4aa8-f2af-dace204b94b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running kernels...\n",
            "\n",
            "Add Kernel:\n",
            "Alpha Kernel (alpha*A + B):\n",
            "Multiplication Kernel (A * B):\n",
            "\n",
            "[Correctness Checks]\n",
            "A + B: True\n",
            "alpha*A + B: True\n",
            "A * B: True\n",
            "[Correctness] Triton result matches PyTorch: False\n",
            "\n",
            "First 5 values after multiplication:\n",
            "alpha      = 3.0\n",
            "a[:5]      = tensor([0.9308, 0.7984, 0.6134, 0.0685, 0.7171])\n",
            "b[:5]      = tensor([0.6817, 0.2635, 0.7879, 0.2374, 0.7572])\n",
            "c[:5]      = tensor([0.6345, 0.2104, 0.4833, 0.0163, 0.5430])\n",
            "(a*b)[:5]  = tensor([0.6345, 0.2104, 0.4833, 0.0163, 0.5430])\n",
            "\n",
            "[Performance]\n",
            "[Perf] Triton: 0.852 ms  (~236.3 GB/s)\n",
            "[Perf] PyTorch: 1.380 ms  (~145.8 GB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLcIFGgK0o26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_RZiPB2y0osT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}